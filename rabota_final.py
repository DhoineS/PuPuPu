# -*- coding: utf-8 -*-
"""Rabota_FINAL.IPYNB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gYPn1HQjkXAdGasiwcjervsPENzG4MHM
"""

from keras.models import Sequential
from keras.layers import Dense , LSTM, Dropout
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import math
import datetime
import openpyxl
from warnings import simplefilter
simplefilter(action="ignore", category=pd.errors.PerformanceWarning)

from google.colab import drive
drive.mount('/content/drive')

DataFrame_Population = pd.read_excel('/content/drive/MyDrive/Prosto_naselenie.xlsx')

DataFrame_Trading = pd.read_excel('/content/drive/MyDrive/Оборот розничной торговли.xlsx')

DataFrame_Work = pd.read_excel('/content/drive/MyDrive/Труд.xlsx')

DataFrame_School = pd.read_excel('/content/drive/MyDrive/Число обучающих заведений.xlsx')

DataFrame_Building = pd.read_excel('/content/drive/MyDrive/Число зданий.xlsx')

DataFrame_MedPerson = pd.read_excel('/content/drive/MyDrive/Численность медперсонала.xlsx')

DataFrame_MeanEarning = pd.read_excel('/content/drive/MyDrive/Среднедушевые Доходы.xlsx')

DataFrame_International_Trading = pd.read_excel('/content/drive/MyDrive/Внешняя торговля Экспорт.xlsx')

DataFrame_VRP = pd.read_excel('/content/drive/MyDrive/Валовой Региональный Продукт.xlsx')

def clean_from_NaN(current_DataFrame):
  current_DataFrame_1= current_DataFrame.apply(pd.to_numeric, args=('coerce',)) # Преобразовать всё в числа, coerce - где не смог - сделай нан
  current_DataFrame_2 = current_DataFrame_1.drop(current_DataFrame_1.columns[current_DataFrame_1.apply(lambda col: col.isnull().any())], axis=1) # удаление столбца, где есть NaN значения
  return current_DataFrame_2

Population = clean_from_NaN(DataFrame_Population)
Trading = clean_from_NaN(DataFrame_Trading)
Work = clean_from_NaN(DataFrame_Work)
School = clean_from_NaN(DataFrame_School)
Building = clean_from_NaN(DataFrame_Building)
MedPerson = clean_from_NaN(DataFrame_MedPerson)
MeanEarning = clean_from_NaN(DataFrame_MeanEarning)
International_Trading = clean_from_NaN(DataFrame_International_Trading)
VRP = clean_from_NaN(DataFrame_VRP)

def splitting_DataFrame_in_normalized(current_DataFrame):
  DataFrame_columns = current_DataFrame.columns.tolist() # Конвертирование всех столбцов в массив, чтобы по индексу обращаться к элементу столбца
  DataFrame_normalized_array = [] # Создание массива, в который будут направляться нормированное содержимое массивов


  for i in range(1,len(DataFrame_columns)):
    vmax = current_DataFrame[DataFrame_columns[i]].max()
    # vmin = current_DataFrame[DataFrame_columns[i]].min()
    #print(f"i={i} max={vmax}")
    # resdiv = (current_DataFrame[DataFrame_columns[i]] - vmin)*0.8  / (vmax-vmin) + 0.3 #
    resdiv = current_DataFrame[DataFrame_columns[i]]/vmax # Обычная нормализация
    # resdiv = current_DataFrame[DataFrame_columns[i]]/(vmax * 1.3) # Нормализация с расширением
    DataFrame_normalized_array.append(resdiv.tolist()) # Нормирование каждого столбца и добавление его в массив, созданный выше

  normalized = '_Нормированная(ый)'
  columns_normalized = []

  for i in range(1 , len(DataFrame_columns)): # Цикл для обозначения нормированных столбцов
    columns_normalized.append(DataFrame_columns[i]+normalized)

  for i in range(0, len(DataFrame_normalized_array)): # Цикл для добавления нормированных массивов в столбцы
    current_DataFrame[columns_normalized[i]] = DataFrame_normalized_array[i]

  return columns_normalized

normalized_names = splitting_DataFrame_in_normalized(Population)

normalized_names = splitting_DataFrame_in_normalized(Trading)

a = len(normalized_names)

normalized_names.insert(0 , 'DataYear')

normalized_names_work = splitting_DataFrame_in_normalized(Work)
normalized_names_pop = splitting_DataFrame_in_normalized(Population)
normalized_names_trading = splitting_DataFrame_in_normalized(Trading)
normalized_names_school = splitting_DataFrame_in_normalized(School)
normalized_names_med = splitting_DataFrame_in_normalized(MedPerson)
normalized_names_meanearn = splitting_DataFrame_in_normalized(MeanEarning)
normalized_names_VRP = splitting_DataFrame_in_normalized(VRP)

normalized_names.insert(0, 'DataYear')

def DataFrame_Factors(a):
  DataFrame_Factor = pd.DataFrame(columns = ['DataYear' , 'Population' , 'Trading' , 'Work' , 'School'  , 'MedPerson' , 'MeanEarning' , 'VRP']) #Создание рядов с такими названиями DataYear можно добавить( временно убрал) , 'Building' , 'International_Trading' , , 'Population' , 'Trading' , 'Work' , 'School'  , 'MedPerson' , 'MeanEarning' , 'VRP'
  # normalized_names = splitting_DataFrame_in_normalized(Population) # Вызов функции с именами и нормированными величинами данного датафрейма
  DataFrame_Factor['Population'] = Population[normalized_names_pop[a]] # Выбор а-того элемента из датафрема и его подставление в указанный ряд
  # normalized_names = splitting_DataFrame_in_normalized(Trading)
  DataFrame_Factor['Trading'] = Trading[normalized_names_trading[a]]
  # normalized_names = splitting_DataFrame_in_normalized(Work)
  DataFrame_Factor['Work'] = Work[normalized_names_work[a]]
  # normalized_names = splitting_DataFrame_in_normalized(School)
  DataFrame_Factor['School'] = School[normalized_names_school[a]]
  # normalized_names = splitting_DataFrame_in_normalized(Building)
  # DataFrame_Factor['Building'] = Building[normalized_names[a]]
  # normalized_names = splitting_DataFrame_in_normalized(MedPerson)
  DataFrame_Factor['MedPerson'] = MedPerson[normalized_names_med[a]]
  # normalized_names = splitting_DataFrame_in_normalized(MeanEarning)
  DataFrame_Factor['MeanEarning'] = MeanEarning[normalized_names_meanearn[a]]
  # normalized_names = splitting_DataFrame_in_normalized(International_Trading)
  # DataFrame_Factor['International_Trading'] = International_Trading[normalized_names[a]]
  # normalized_names = splitting_DataFrame_in_normalized(VRP)
  DataFrame_Factor['VRP'] = VRP[normalized_names_VRP[a]]
  DataFrame_Factor['DataYear'] = [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020 , 2021, 2022]
  DF = DataFrame_Factor.drop([22])
  return DF

s_copy = DataFrame_Factors(0).copy( deep = 'True')
a = s_copy.diff()
s_copy

a

TIME_DELTA = 5 # глубина нейронной сети по времени
TIME_YOFFSET = 2
SAMPLES = 10
Training_regions = 81 # Количество регионов для трейнингового датасета
X_train_all = []
y_train_all =[]

for i in range(Training_regions):
  DataFrame = DataFrame_Factors(i)
  del DataFrame['DataYear']
  DataFrame_diff_full = DataFrame.diff()# Добавляю производую, чтобы не было заскоков
  DataFrame_diff = DataFrame_diff_full.drop([0])# Сбрасываю первую строку, потому что производная в 0 = NaN
  X_train=[]

  for index in range(len(DataFrame_diff)):
    row = DataFrame_diff.iloc[index]
    X_train.append(row.to_list())
  region_X = X_train

  for j in range(SAMPLES):
    start = j
    stop = start+TIME_DELTA
    sample_X = region_X[start:stop]
    sample_Y = region_X[stop+TIME_YOFFSET]
    np_sample_X = np.array(sample_X)
    np_sample_Y = np.array(sample_Y)
    X_train_all.append(np_sample_X)
    y_train_all.append(np_sample_Y)

np_X_train_all = np.array(X_train_all)#np.array([])
np_y_train_all  = np.array(y_train_all)#np.array([])

print(np.shape(np_X_train_all))
print(np.shape(np_y_train_all))

np_X_train_all = np.array(X_train_all)
np_y_train_all = np.array(y_train_all)

TIME_DELTA_test = 5 # глубина нейронной сети по времени
TIME_YOFFSET_test = 2
SAMPLES_test = 10
Test_regions = range(81,90) # Выборка регионов для валидации нейронной сети
X_test_all = []
y_test_all = []

for i in Test_regions:
  DataFrame = DataFrame_Factors(i)
  del DataFrame['DataYear']
  DataFrame_diff_full = DataFrame.diff()# Добавляю производую, чтобы не было заскоков
  DataFrame_diff = DataFrame_diff_full.drop([0])# Сбрасываю первую строку, потому что производная в 0 = NaN
  X_test=[]

  for index in range(len(DataFrame_diff)):
    row = DataFrame_diff.iloc[index]
    X_test.append(row.to_list())
  region_X = X_test

  for j in range(SAMPLES_test):
    start = j
    stop = start+TIME_DELTA_test
    sample_X = region_X[start:stop]
    sample_Y = region_X[stop+TIME_YOFFSET_test]
    np_sample_X= np.array(sample_X)
    np_sample_Y= np.array(sample_Y)
    X_test_all.append(np_sample_X)
    y_test_all.append(np_sample_Y)

np_X_test_all = np.array(X_test_all)
np_y_test_all = np.array(y_test_all)

X_train_all=list(np_X_train_all)
for i in range(0,len(X_train_all)):
  X_train_all[i]=np.reshape(X_train_all[i],5*7)
X_train_all=np.array(X_train_all)
X_test_all=list(np_X_test_all)
for i in range(0,len(X_test_all)):
  X_test_all[i]=np.reshape(X_test_all[i],5*7)
X_test_all=np.array(X_test_all)
Y_test_all=np_y_test_all
Y_train_all=np_y_train_all

model = Sequential()
model.add(Dense(200, activation='tanh', input_shape=(5 * 7,)))
model.add(Dropout(0.5))
model.add(Dense(100, activation='tanh'))
model.add(Dropout(0.25))
model.add(Dense(50, activation='tanh'))
model.add(Dropout(0.125))
model.add(Dense(7, activation='tanh'))


model.compile(optimizer='adam', # rmsprop
              loss = "mean_squared_error")


history = model.fit(X_train_all, Y_train_all,
          batch_size=10,
          epochs=300,
          verbose=1,validation_data=(X_test_all, Y_test_all))

# model = Sequential()
# model.add(Dense(200, activation='tanh', input_shape=(5 * 7,)))
# model.add(Dropout(0.5))
# model.add(Dense(100, activation='tanh'))
# model.add(Dropout(0.25))
# model.add(Dense(50, activation='tanh'))
# model.add(Dropout(0.125))
# model.add(Dense(7, activation='tanh'))


# model.compile(optimizer='adam',
#               loss = "mean_squared_error") # metrics = "mean_squared_error" # optimizer = 'adam'


# history_2 = model.fit(X_train_all, Y_train_all,
#           batch_size=32,
#           epochs=300,
#           verbose=1,validation_data=(X_test_all, Y_test_all))

# #Build neural network
# model = Sequential()
# model.add(LSTM(6*9,input_shape=(5, 7), return_sequences=True))#, return_sequences=True
# model.add(Dropout(0.5))
# model.add(LSTM(6*9))
# model.add(Dropout(0.25))
# # model.add(LSTM(6*9))
# # model.add(Dropout(0.125))
# model.add(Dense(7, activation='sigmoid'))

# # Compile model
# model.compile(optimizer = "rmsprop",
#               loss = "mean_squared_error")

# # Train model
# history = model.fit(np_X_train_all, np_y_train_all,
#           batch_size=10,
#           epochs=300,
#           verbose=1,validation_data=(np_X_test_all, np_y_test_all))

history_val=history
val = history_val.history['val_loss']
accuracy = history.history['loss']
plt.title('Валидационная (синяя) обучающая (оранжевая)')
plt.plot(val)
plt.plot(accuracy)

# history_val_2=history_2
# val_2 = history_val_2.history['val_loss']
# accuracy_2 = history_2.history['loss']
# plt.title('Валидационная (синяя) обучающая (оранжевая)')
# plt.plot(val_2)
# plt.plot(accuracy_2)

index_reg = Population.columns.get_loc('Ленинградская область')
print("Index of {} column in given dataframe is : {}".format('Ленинградская область', index_reg))
# Нахождение индекса + 1 (то есть Псковская область это 31) региона по названию

index_reg = MeanEarning.columns.get_loc('Псковская область')
print("Index of {} column in given dataframe is : {}".format('Псковская область', index_reg))
# Нахождение индекса + 1 (то есть Псковская область это 31) региона по названию

DataFrame_copy = DataFrame_Factors(30).copy(deep = True)
DataFrame_copy_diff_full = DataFrame_copy.diff()
DataFrame_copy_diff = DataFrame_copy_diff_full.drop([0])
del DataFrame_copy_diff['DataYear']
DataFrame_copy.plot(x = 'DataYear' , y = 'Population')

DataFrame_copy_diff

DataYear = pd.DataFrame({'DataYear' : [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021]})
DataFrame_copy_diff_pred_full = pd.concat([DataYear , DataFrame_copy_diff] , axis = 1)
DataFrame_copy_diff_pred = DataFrame_copy_diff_pred_full.drop([0])
DataFrame_copy_diff_pred

DataFrame_copy_diff_pred.plot(x = 'DataYear' , y = 'Population') # DataFrame_copy_diff_pred.plot(x = 'DataYear' , y = 'Population') # DataFrame_copy.plot(x = 'DataYear' , y = 'Popultaion')

# DataFrame_pred = DataFrame_copy_diff_pred.iloc[:20] # DataFrame_Copy_diff_pred.iloc[:20] # _diff_pred
# DataFrame_pred_test = DataFrame_pred.loc[:, DataFrame_pred.columns != 'DataYear']
# DataFrame_pred_test
DataFrame_pred = DataFrame_copy_diff_pred.iloc[:10] # DataFrame_Copy_diff_pred.iloc[:20] # _diff_pred
DataFrame_pred_test = DataFrame_pred.loc[:, DataFrame_pred.columns != 'DataYear']
DataFrame_pred_test
# DataFrame_changing_pred_earn = pd.DataFrame({'Changing' : [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]})
# DataFrame_changing_pred_VRP = pd.DataFrame({'Changing' : [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]})
# DataFrame_pred_test['MeanEarning'] = DataFrame_pred_test['MeanEarning'] + DataFrame_changing_pred_earn['Changing']
# DataFrame_pred_test['VRP'] = DataFrame_pred_test['VRP'] + DataFrame_changing_pred_VRP['Changing']
# DataFrame_pred_test

X_pred = []
TIME_DELTA_test = 5 # глубина нейронной сети по времени
TIME_YOFFSET_test = 2
SAMPLES_test = 6
X_pred_test=[]
for index in range(len(DataFrame_pred_test)):
  row = DataFrame_pred_test.iloc[index]
  X_pred_test.append(row.to_list())
region_X_pred = X_pred_test
for j in range(SAMPLES_test):
  # start = random.randint(0,len(region_X)-TIME_DELTA_test - TIME_YOFFSET_test-1)
  start = j
  stop = start+TIME_DELTA_test
  sample_X_pred = region_X_pred[start:stop]
  np_sample_X_pred = np.array(sample_X_pred)
  X_pred.append(np_sample_X_pred)

X_pred =list(X_pred)
for i in range(len(X_pred)):
  X_pred[i] = np.reshape(X_pred[i], 5*7)
X_pred = np.array(X_pred)

DataFrame_Factors(28).plot(x = 'DataYear' , y = 'Population')

corr_matrix = DataFrame_Factors(30).corr()

print(len(X_pred))

predictions = model.predict(X_pred) # до 3 , чтобы модель прогнозировала с

print(predictions)

df_res = DataFrame_pred.iloc[:10]
for i in range(0,len(predictions)):
  df_ins = pd.DataFrame({'DataYear' : 2011 +i, 'Population': predictions[i,0], 'Trading' : predictions[i,1] , 'Work' : predictions[i,2] , 'School' : predictions[i,3], 'MedPerson' : predictions[i,4] , 'MeanEarning' : predictions[i,5]  , 'VRP' : predictions[i,6]},index  = [6])
  df_res = pd.concat([df_res, df_ins], ignore_index = True)
df_res
# df_res['Population'] = df_res['Population'] * DataFrame_Population['Псковская область'].max()
#, 'Building' : predictions[i,4] , , 'International_Trading' : predictions[i,7] , 'Trading' : predictions[i,1] , 'Work' : predictions[i,2] , 'School' : predictions[i,3], 'MedPerson' : predictions[i,4] , 'MeanEarning' : predictions[i,5]  , 'VRP' : predictions[i,6]

df_res.plot(x = 'DataYear' , y = 'Population')

df_res_copy = df_res.copy(deep = 'True')

df_integr = df_res_copy.iloc[10:]
df_integr

diff_pred = []
for index in range(len(df_integr)):
    row = df_integr.iloc[index]
    diff_pred.append(row.tolist())
diff_pred[0][0]

value_row = DataFrame_Factors(30).iloc[9]
value_list = value_row.tolist()
value_list

DataFrame_integr_pred = DataFrame_Factors(30).iloc[:9]
ar_pop = [value_list[1]]
ar_trading = [value_list[2]]
ar_work = [value_list[3]]
ar_school = [value_list[4]]
ar_medperson = [value_list[5]]
ar_meanearning = [value_list[6]]
ar_vrp = [value_list[7]]
for i in range(len(diff_pred)):
    summ_pop = ar_pop[i] + diff_pred[i][1]
    ar_pop.append(summ_pop)
    summ_trading = ar_trading[i] + diff_pred[i][2]
    ar_trading.append(summ_trading)
    summ_work = ar_work[i] + diff_pred[i][3]
    ar_work.append(summ_work)
    summ_school = ar_school[i] + diff_pred[i][4]
    ar_school.append(summ_school)
    summ_medperson = ar_school[i] + diff_pred[i][5]
    ar_medperson.append(summ_medperson)
    summ_meanearning = ar_meanearning[i] + diff_pred[i][6]
    ar_meanearning.append(summ_meanearning)
    summ_vrp = ar_vrp[i] + diff_pred[i][7]
    ar_vrp.append(summ_vrp)
    last = pd.DataFrame({'DataYear' : 2009+i , 'Population' : ar_pop[i] , 'Trading' : ar_trading[i] , 'Work' : ar_work[i] , 'School' : ar_school[i] , 'MedPerson' : ar_medperson[i] , 'MeanEarning': ar_meanearning[i], 'VRP': ar_vrp[i]} , index = [7])
    DataFrame_integr_pred = pd.concat([DataFrame_integr_pred , last] , ignore_index = True)

DataFrame_integr_pred

max_population = DataFrame_Population['Псковская область'].max() # Население
DataFrame_integr_pred_max = DataFrame_integr_pred['Population'] * max_population # Население
# max_school = DataFrame_School['Ленинградская область'].max()
# DataFrame_integr_pred_max = DataFrame_integr_pred['School'] * max_school # Численность людей 4.5. ЧИСЛЕННОСТЬ ОБУЧАЮЩИХСЯ ОРГАНИЗАЦИЙ, ОСУЩЕСТВЛЯЮЩИХ ОБРАЗОВАТЕЛЬНУЮ ДЕЯТЕЛЬНОСТЬ ПО ОБРАЗОВАТЕЛЬНЫМ ПРОГРАММАМ НАЧАЛЬНОГО, ОСНОВНОГО И СРЕДНЕГО ОБЩЕГО ОБРАЗОВАНИЯ1)
# plot = DataFrame_Factors(31).copy
# plot = pl
DataFrame_integr_pred_max

DataFrame_plot = DataFrame_Factors(30).iloc[:15].copy(deep = 'True')
DataFrame_plot_pop = DataFrame_plot['Population'] * max_population
DataFrame_plot_pop

DataFrame_plot_pop

DataFrame_integr_pred_max

x_plot_1 = []
x_plot_pred_1 = []
y_plot_1  = DataFrame_plot_pop
y_plot_pred_1 = DataFrame_integr_pred_max
for i in range(15):
    x_plot_pred_1.append(2000+i)
for i in range(15):
    x_plot_1.append(2000+i)
print(x_plot_1)
plt.ylabel('тыс.чел')
plt.xlabel('Года')
plt.plot(x_plot_pred_1, y_plot_pred_1 , x_plot_1 , y_plot_1)

x_plot = []
x_plot_pred = []
y_plot  = DataFrame_plot_pop
y_plot_pred = DataFrame_integr_pred_max
for i in range(31):
    x_plot_pred.append(2000+i)
for i in range(20):
    x_plot.append(2000+i)
print(x_plot)
plt.ylabel('Обуч. тыс. чел')
plt.xlabel('Года')
plt.plot(x_plot_pred, y_plot_pred , x_plot , y_plot)